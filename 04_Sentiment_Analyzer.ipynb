{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer\n",
    "Download the data from this [review](http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html), [stop words](http://www.lextek.com/manuals/onix/stopwords1.html)\n",
    "Inspired from this [NLP Course](https://deeplearningcourses.com/c/data-science-natural-language-processing-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from future.utils import iteritems\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stopwords\n",
    "# You can also use an alternative source of stopwords:\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# stopwords = stopwords.words('english')\n",
    "stopwords = set(w.rstrip() for w in open('./tmp/dataset/stopwords.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reviews\n",
    "positive_reviews = BeautifulSoup(open('./tmp/dataset/sorted_data_acl/electronics/positive.review').read())\n",
    "positive_reviews = positive_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews = BeautifulSoup(open('./tmp/dataset/sorted_data_acl/electronics/negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There arte more positive reviews than negative reviews\n",
    "# So let's take a random sample so we have balanced classes\n",
    "# np.random.shuffle(positive_reviews)\n",
    "# positive_reviews = positive_reviews[:len(negative_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of positive reviews is:  1000\n",
      "Length of negative reviiews is:  1000\n",
      "Diff is:  0\n"
     ]
    }
   ],
   "source": [
    "# We can also oversample the negative reviews\n",
    "diff = len(positive_reviews) - len(negative_reviews)\n",
    "idxs = np.random.choice(len(negative_reviews), size=diff)\n",
    "extra = [negative_reviews[i] for i in idxs]\n",
    "negative_reviews += extra\n",
    "print(\"Length of positive reviews is: \", len(positive_reviews))\n",
    "print(\"Length of negative reviiews is: \", len(negative_reviews))\n",
    "print('Diff is: ', diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'purchased',\n",
       " 'this',\n",
       " 'unit',\n",
       " 'due',\n",
       " 'to',\n",
       " 'frequent',\n",
       " 'blackouts',\n",
       " 'in',\n",
       " 'my',\n",
       " 'area',\n",
       " 'and',\n",
       " '2',\n",
       " 'power',\n",
       " 'supplies',\n",
       " 'going',\n",
       " 'bad',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'run',\n",
       " 'my',\n",
       " 'cable',\n",
       " 'modem',\n",
       " ',',\n",
       " 'router',\n",
       " ',',\n",
       " 'PC',\n",
       " ',',\n",
       " 'and',\n",
       " 'LCD',\n",
       " 'monitor',\n",
       " 'for',\n",
       " '5',\n",
       " 'minutes',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'more',\n",
       " 'than',\n",
       " 'enough',\n",
       " 'time',\n",
       " 'to',\n",
       " 'save',\n",
       " 'work',\n",
       " 'and',\n",
       " 'shut',\n",
       " 'down',\n",
       " '.',\n",
       " 'Equally',\n",
       " 'important',\n",
       " ',',\n",
       " 'I',\n",
       " 'know',\n",
       " 'that',\n",
       " 'my',\n",
       " 'electronics',\n",
       " 'are',\n",
       " 'receiving',\n",
       " 'clean',\n",
       " 'power',\n",
       " '.',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'that',\n",
       " 'this',\n",
       " 'investment',\n",
       " 'is',\n",
       " 'minor',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'the',\n",
       " 'loss',\n",
       " 'of',\n",
       " 'valuable',\n",
       " 'data',\n",
       " 'or',\n",
       " 'the',\n",
       " 'failure',\n",
       " 'of',\n",
       " 'equipment',\n",
       " 'due',\n",
       " 'to',\n",
       " 'a',\n",
       " 'power',\n",
       " 'spike',\n",
       " 'or',\n",
       " 'an',\n",
       " 'irregular',\n",
       " 'power',\n",
       " 'supply',\n",
       " '.',\n",
       " 'As',\n",
       " 'always',\n",
       " ',',\n",
       " 'Amazon',\n",
       " 'had',\n",
       " 'it',\n",
       " 'to',\n",
       " 'me',\n",
       " 'in',\n",
       " '<',\n",
       " '2',\n",
       " 'business',\n",
       " 'days']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = positive_reviews[0]\n",
    "nltk.tokenize.word_tokenize(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a function that does some pre-processing for us\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word-to-index map so that we can create our word-frequency vectors later\n",
    "# let's also save the tokenized versions so we don't have to tokenize again later\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "orig_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "for review in positive_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "            \n",
    "print(len(orig_reviews))\n",
    "print(len(positive_tokenized))\n",
    "\n",
    "for review in negative_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_index_map): 11078\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(\"len(word_index_map):\", len(word_index_map))\n",
    "print(len(orig_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create our input matrices\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # normalize it before setting label\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "print(N)\n",
    "# (N x D+1 matrix - keeping them together for now so we can shuffle more easily later\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# shuffle the data and create train/test splits\n",
    "# try it multiple times!\n",
    "print(len(orig_reviews))\n",
    "print(len(data))\n",
    "orig_reviews, data = shuffle(orig_reviews, data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "# last 100 rows will be test\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/longxiajun/MySoftware/anaconda3/envs/CV/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.7894736842105263\n",
      "Test Accuracy:  0.73\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# model = AdaBoostClassifier()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Train Accuracy: \", model.score(Xtrain, Ytrain))\n",
    "print(\"Test Accuracy: \", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit -0.6857095404511567\n",
      "bad -0.7751270487824288\n",
      "cable 0.6617912018414011\n",
      "time -0.7441274137679607\n",
      "'ve 0.7887614816721545\n",
      "month -0.7443900978808807\n",
      "sound 1.0635737835662251\n",
      "lot 0.7265657696167179\n",
      "you 0.9648821927170496\n",
      "n't -1.886744270201093\n",
      "easy 1.8075354817028313\n",
      "quality 1.3865465316642431\n",
      "company -0.5264810190187412\n",
      "item -0.9285575988458221\n",
      "wa -1.5588968153619203\n",
      "perfect 1.0168164134407585\n",
      "fast 0.8925081978859608\n",
      "ha 0.8231246504425964\n",
      "price 2.768678345029652\n",
      "value 0.507267675189047\n",
      "money -1.1146868675682107\n",
      "memory 0.9914582512972566\n",
      "picture 0.5977102546395987\n",
      "buy -0.8834390753979323\n",
      "bit 0.6199125029091249\n",
      "happy 0.6574491982865284\n",
      "pretty 0.7171489748201827\n",
      "doe -1.2405713999845693\n",
      "highly 0.9655071461824608\n",
      "recommend 0.6051997407180466\n",
      "fit 0.533200648559974\n",
      "customer -0.6813954386768624\n",
      "support -0.8635750048213835\n",
      "little 0.9555952489722748\n",
      "sent -0.5056945005642455\n",
      "returned -0.7989581125273378\n",
      "excellent 1.382061291735626\n",
      "love 1.2161679527203537\n",
      "home 0.5673471842933068\n",
      "week -0.7279920801265377\n",
      "using 0.6465396988332576\n",
      "video 0.5427092040370962\n",
      "poor -0.7654827870930212\n",
      "look 0.5055350814519886\n",
      "then -1.1196223226323254\n",
      "tried -0.8119321316138141\n",
      "try -0.6908619113434247\n",
      "space 0.5520049698900112\n",
      "comfortable 0.6783520698860147\n",
      "hour -0.6241224092359767\n",
      "expected 0.5700392212120042\n",
      "speaker 0.8864353972011659\n",
      "cheap -0.5208227478493612\n",
      "stopped -0.5211186922475621\n",
      "junk -0.5453550665355829\n",
      "paper 0.5155324466282888\n",
      "terrible -0.5128626707964729\n",
      "return -1.1975975424539205\n",
      "waste -0.973528010628952\n",
      "refund -0.5783233830876534\n"
     ]
    }
   ],
   "source": [
    "# let's look at the weights for each word\n",
    "# try it with different threshold values! \n",
    "# This is used for logistic regresssion\n",
    "threshold = 0.5\n",
    "for word, index in iteritems(word_index_map):\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most wrong positive review (prob = 0.3597615153406063, pred = 0.0):\n",
      "\n",
      "A device like this either works or it doesn't.  This one happens to work\n",
      "\n",
      "Most wrong negative review (prob = 0.6009433857348249, pred = 1.0):\n",
      "\n",
      "The Voice recorder meets all my expectations and more\n",
      "Easy to use, easy to transfer great results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check misclassified examples\n",
    "preds = model.predict(X)\n",
    "P = model.predict_proba(X)[:,1] # p(y = 1 | x)\n",
    "\n",
    "# since there are many, just print the \"most\" wrong samples\n",
    "minP_whenYis1 = 1\n",
    "maxP_whenYis0 = 0\n",
    "wrong_positive_review = None\n",
    "wrong_negative_review = None\n",
    "wrong_positive_prediction = None\n",
    "wrong_negative_prediction = None\n",
    "for i in range(N):\n",
    "    p = P[i]\n",
    "    y = Y[i]\n",
    "    if y == 1 and p < 0.5:\n",
    "        if p < minP_whenYis1:\n",
    "            wrong_positive_review = orig_reviews[i]\n",
    "            wrong_positive_prediction = preds[i]\n",
    "            minP_whenYis1 = p\n",
    "    elif y == 0 and p > 0.5:\n",
    "        if p > maxP_whenYis0:\n",
    "            wrong_negative_review = orig_reviews[i]\n",
    "            wrong_negative_prediction = preds[i]\n",
    "            maxP_whenYis0 = p\n",
    "\n",
    "print(\"Most wrong positive review (prob = %s, pred = %s):\" % (minP_whenYis1, wrong_positive_prediction))\n",
    "print(wrong_positive_review)\n",
    "print(\"Most wrong negative review (prob = %s, pred = %s):\" % (maxP_whenYis0, wrong_negative_prediction))\n",
    "print(wrong_negative_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
